{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72279455-8eda-4c04-baa9-ae2f7c9f04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theory\n",
    "Logistic Regression:\n",
    "                    Logistic Regression is a supervised learning algorithm used for binary classification (predicting 0 or 1, Yes or No).\n",
    "It estimates the probability that a given input belongs to a particular class.\n",
    "\n",
    "Where:\n",
    "      . p is the predicted probability of the positive class\n",
    "\n",
    "      . ùõΩ0,ùõΩ1,‚Ä¶,ùõΩùëõ are the coefficients\n",
    "\n",
    "Applications: Spam detection, disease diagnosis, customer churn prediction, etc.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6630e51e-19a9-44b1-b02d-a7198f2667b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-5.95635054  1.76904586]\n",
      "Predicted probabilities: [0.00258266 0.99060301]\n",
      "Predicted classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "#Scratch code\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "# Adding bias term\n",
    "X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "\n",
    "# Gradient Descent parameters\n",
    "theta = np.zeros(X_b.shape[1])\n",
    "learning_rate = 0.1\n",
    "iterations = 1000\n",
    "\n",
    "# Gradient Descent\n",
    "for _ in range(iterations):\n",
    "    z = X_b.dot(theta)\n",
    "    h = sigmoid(z)\n",
    "    gradient = X_b.T.dot(h - y) / len(y)\n",
    "    theta -= learning_rate * gradient\n",
    "\n",
    "print(\"Coefficients:\", theta)\n",
    "\n",
    "# Prediction\n",
    "X_new = np.array([[0], [6]])\n",
    "X_new_b = np.c_[np.ones((len(X_new), 1)), X_new]\n",
    "y_pred = sigmoid(X_new_b.dot(theta))\n",
    "print(\"Predicted probabilities:\", y_pred)\n",
    "print(\"Predicted classes:\", (y_pred >= 0.5).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b7dab6-3e54-44dd-9d7c-8b82fab01f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -3.7481774320466803\n",
      "Coefficient: 1.0470438047457902\n",
      "Predicted probabilities: [0.02301832 0.92649706]\n",
      "Predicted classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = X.reshape(-1, 1)  # sklearn expects 2D input\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Intercept:\", model.intercept_[0])\n",
    "print(\"Coefficient:\", model.coef_[0][0])\n",
    "\n",
    "# Prediction\n",
    "y_prob = model.predict_proba(np.array([[0], [6]]))[:, 1]\n",
    "y_class = model.predict(np.array([[0], [6]]))\n",
    "print(\"Predicted probabilities:\", y_prob)\n",
    "print(\"Predicted classes:\", y_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4ce41-d07b-4683-856d-d45a6cf4fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "THEORY\n",
    "Linear Regression:\n",
    "                  Linear Regression is a supervised learning algorithm used to predict a continuous target variable.\n",
    "\n",
    "The formula is:\n",
    "                  y=Œ≤0+Œ≤1x1+Œ≤2x2+...+Œ≤nxn+œµ\n",
    "Where:\n",
    "     .  Œ≤0,Œ≤1,......,Œ≤n are the coefficients\n",
    "     .  œµ is the error term\n",
    "\n",
    "Applications: Predicting house prices, sales forecasting, stock price prediction, etc.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bcebb52-5ad5-4ac9-b381-e163c3a18778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (b0): 2.200000000000004\n",
      "Slope (b1): 0.6000000000000005\n",
      "Predictions: [2.2 5.8]\n"
     ]
    }
   ],
   "source": [
    "#Scratch code\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Adding bias term (intercept)\n",
    "X_b = np.c_[np.ones((len(X), 1)), X]  # shape (n_samples, 2)\n",
    "\n",
    "# Normal Equation: theta = (X^T * X)^-1 * X^T * y\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(\"Intercept (b0):\", theta_best[0])\n",
    "print(\"Slope (b1):\", theta_best[1])\n",
    "\n",
    "# Prediction\n",
    "X_new = np.array([[0], [6]])\n",
    "X_new_b = np.c_[np.ones((len(X_new), 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "print(\"Predictions:\", y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aa0264e-3eda-45f7-adcf-d349225bc2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 2.2\n",
      "Coefficient: 0.6\n",
      "Predictions: [2.2 5.8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Reshape X to 2D for sklearn\n",
    "X = X.reshape(-1, 1)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(np.array([[0], [6]]))\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b70aa2-d69b-420a-b576-76810e63d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theory\n",
    "Decision Tree:\n",
    "            Decision Tree is a supervised learning algorithm that can be used for classification or regression.\n",
    "            It splits the dataset into subsets based on feature values.\n",
    "            Each internal node represents a feature, each branch represents a decision rule, and each leaf node represents the output / class.\n",
    "It follows a tree structure.\n",
    "\n",
    "Applications: Customer segmentation, loan approval, medical diagnosis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba1a8b5-5113-43d3-a737-332bab7804a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Simple decision tree based on threshold\n",
    "class SimpleDecisionTree:\n",
    "    def fit(self, X, y):\n",
    "        # For simplicity, find threshold to split data\n",
    "        thresholds = np.unique(X)\n",
    "        best_thresh, best_score = None, 0\n",
    "        for t in thresholds:\n",
    "            left = y[X[:,0] <= t]\n",
    "            right = y[X[:,0] > t]\n",
    "            score = sum([max(Counter(left).values(), default=0), max(Counter(right).values(), default=0)])\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_thresh = t\n",
    "        self.threshold = best_thresh\n",
    "        # Determine majority class for each side\n",
    "        self.left_class = Counter(y[X[:,0] <= self.threshold]).most_common(1)[0][0]\n",
    "        self.right_class = Counter(y[X[:,0] > self.threshold]).most_common(1)[0][0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.left_class if x[0] <= self.threshold else self.right_class for x in X])\n",
    "\n",
    "# Train and predict\n",
    "tree = SimpleDecisionTree()\n",
    "tree.fit(X, y)\n",
    "y_pred = tree.predict(X)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b82a6cd-df64-4345-b7aa-72fe656399f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7f59f-39ab-45eb-91f4-2e12e6b1954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "THEORY\n",
    "Random Forest:\n",
    "              Random Forest is a supervised learning algorithm used for classification and regression.\n",
    "\n",
    "              It combines multiple decision trees to make more accurate and stable predictions.\n",
    "\n",
    "              Each tree is trained on a random subset of the data and features (bootstrap sampling).\n",
    "\n",
    "              The final prediction is based on majority voting (classification) or average (regression).\n",
    "\n",
    "Applications: Fraud detection, stock market prediction, medical diagnosis, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9898f5fa-22c7-4c60-bd87-ce1a5a33017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Simple Random Forest\n",
    "class SimpleRandomForest:\n",
    "    def __init__(self, n_trees=3):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        for _ in range(self.n_trees):\n",
    "            # Bootstrap sample\n",
    "            idx = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            tree = DecisionTreeClassifier()\n",
    "            tree.fit(X[idx], y[idx])\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Collect predictions from all trees\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Majority vote\n",
    "        y_pred = [Counter(tree_preds[:,i]).most_common(1)[0][0] for i in range(X.shape[0])]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "# Train and predict\n",
    "rf = SimpleRandomForest(n_trees=3)\n",
    "rf.fit(X, y)\n",
    "y_pred = rf.predict(X)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6cd9688-df59-493c-9339-f506835fed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=3, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660c691-f001-4def-ba52-64ebaf6f267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theory \n",
    "Support Vector Machine (SVM):\n",
    "                      Support Vector Machine is a supervised learning algorithm used for classification and regression, but mostly for classification.\n",
    "\n",
    "                      It tries to find the hyperplane that best separates the classes with the maximum margin.\n",
    "\n",
    "                      Support vectors are the data points closest to the hyperplane, which influence its position.\n",
    " \n",
    "                      Can handle linear and non-linear data using kernel functions.\n",
    "\n",
    "Applications: Face recognition, text classification, email spam detection, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22067294-a0d1-443e-9716-80a4cf8fdd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([-1, -1, 1, 1, 1])  # SVM expects labels as -1 and 1\n",
    "\n",
    "# Simple linear SVM using gradient descent (primal form)\n",
    "class SimpleSVM:\n",
    "    def __init__(self, lr=0.01, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y[idx]))\n",
    "                    self.b -= self.lr * y[idx]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) + self.b\n",
    "        return np.sign(approx)\n",
    "\n",
    "# Train and predict\n",
    "svm = SimpleSVM()\n",
    "svm.fit(X, y)\n",
    "y_pred = svm.predict(X)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5284d4d-444d-4b9c-8d23-383160bac8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 1, 1, 1])  # sklearn can use 0/1 labels\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5ee2a-79ff-451f-9886-def93d3e682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theory\n",
    "K-Nearest Neighbors (KNN):\n",
    "                          Theory: K-Nearest Neighbors (KNN)\n",
    "\n",
    "                        K-Nearest Neighbors is a supervised learning algorithm used for classification and regression.\n",
    "\n",
    "                        It predicts the label of a new data point based on the majority class (classification) or average value (regression) of its k\n",
    "                        nearest neighbors in the training set.\n",
    "\n",
    "                        Distance metric (e.g., Euclidean distance) is used to find the nearest neighbors.\n",
    "\n",
    "Applications: Handwriting recognition, recommendation systems, image classification, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a1bb242-6e33-4711-932c-ee1a8dba9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Sample dataset\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# Simple KNN\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test, k=3):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        # Compute distances\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in X_train]\n",
    "        # Get k nearest neighbors\n",
    "        k_idx = np.argsort(distances)[:k]\n",
    "        k_labels = y_train[k_idx]\n",
    "        # Majority vote\n",
    "        most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "        y_pred.append(most_common)\n",
    "    return np.array(y_pred)\n",
    "\n",
    "# Prediction\n",
    "X_test = np.array([[0], [6]])\n",
    "y_pred = knn_predict(X_train, y_train, X_test, k=3)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592abaf6-9571-4108-8cdc-a3759aef55d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "X_test = np.array([[0], [6]])\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556e26e-c098-4970-9578-02ac76dc63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theory\n",
    "Naive Bayes:\n",
    "\n",
    "            Naive Bayes is a supervised learning algorithm based on Bayes‚Äô Theorem.\n",
    "\n",
    "            It assumes that all features are independent of each other (the ‚Äúnaive‚Äù assumption).\n",
    "\n",
    "             It calculates the probability of each class given the input features and predicts the class with highest probability.\n",
    "Where:\n",
    "      P(C‚à£X) = Probability of class C given features X\n",
    "      P(X‚à£C) = Probability of features X given class C\n",
    "      P(C) = Prior probability of class C\n",
    "      P(X) = Probability of features X\n",
    "Applications: Spam detection, sentiment analysis, document classification, medical diagnosis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f2d2e5-08bd-49ad-859b-f84b5b4918ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X_train = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])\n",
    "y_train = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "class GaussianNB:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "        self.prior = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.mean[c] = X_c.mean(axis=0)\n",
    "            self.var[c] = X_c.var(axis=0)\n",
    "            self.prior[c] = X_c.shape[0] / X.shape[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "        for c in self.classes:\n",
    "            prior = np.log(self.prior[c])\n",
    "            likelihood = -0.5 * np.sum(np.log(2 * np.pi * self.var[c])) \\\n",
    "                         -0.5 * np.sum((x - self.mean[c])**2 / self.var[c])\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "# Train and predict\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "X_test = np.array([[0], [6]])\n",
    "y_pred = nb.predict(X_test)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef53a2c-787b-4b24-b1b0-f9dc99b312af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])\n",
    "y_train = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "X_test = np.array([[0], [6]])\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f50aa2c-8a88-4303-9f94-6112b5ffb98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
